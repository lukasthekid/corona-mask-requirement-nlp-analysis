{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c65e914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "from collections import Counter \n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d12d74",
   "metadata": {},
   "source": [
    "Lemmatizing was used in the Preprocessing now we will do word embedding\n",
    "https://stackoverflow.com/questions/23877375/word2vec-lemmatization-of-corpus-before-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f8de2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103, 3)\n",
      "ID_GodotObject     int64\n",
      "merged_text       object\n",
      "tokens            object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_GodotObject</th>\n",
       "      <th>merged_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000115059032</td>\n",
       "      <td>﻿20sars-cov-2maskenpflicht medizinisch_Persona...</td>\n",
       "      <td>['\\ufeff20sars-cov-2maskenpflicht', 'medizinis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000116305030</td>\n",
       "      <td>﻿20_Coronavirus schrittweise Einführung Masken...</td>\n",
       "      <td>['\\ufeff20_Coronavirus', 'schrittweise', 'Einf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000116325081</td>\n",
       "      <td>﻿20kein Ende Sicht Regierung setzen Maske bei ...</td>\n",
       "      <td>['\\ufeff20kein', 'Ende', 'Sicht', 'Regierung',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000116346340</td>\n",
       "      <td>﻿20_Coronavirus Maskenpflicht Supermarkt späte...</td>\n",
       "      <td>['\\ufeff20_Coronavirus', 'Maskenpflicht', 'Sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000116371728</td>\n",
       "      <td>﻿20_Coronavirus Sonderbeauftragter Clemens Aue...</td>\n",
       "      <td>['\\ufeff20_Coronavirus', 'Sonderbeauftragter',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_GodotObject                                        merged_text  \\\n",
       "0   2000115059032  ﻿20sars-cov-2maskenpflicht medizinisch_Persona...   \n",
       "1   2000116305030  ﻿20_Coronavirus schrittweise Einführung Masken...   \n",
       "2   2000116325081  ﻿20kein Ende Sicht Regierung setzen Maske bei ...   \n",
       "3   2000116346340  ﻿20_Coronavirus Maskenpflicht Supermarkt späte...   \n",
       "4   2000116371728  ﻿20_Coronavirus Sonderbeauftragter Clemens Aue...   \n",
       "\n",
       "                                              tokens  \n",
       "0  ['\\ufeff20sars-cov-2maskenpflicht', 'medizinis...  \n",
       "1  ['\\ufeff20_Coronavirus', 'schrittweise', 'Einf...  \n",
       "2  ['\\ufeff20kein', 'Ende', 'Sicht', 'Regierung',...  \n",
       "3  ['\\ufeff20_Coronavirus', 'Maskenpflicht', 'Sup...  \n",
       "4  ['\\ufeff20_Coronavirus', 'Sonderbeauftragter',...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/p_content.csv')\n",
    "df1 = df[['ID_GodotObject','merged_text','tokens']]\n",
    "print(df1.shape)\n",
    "print(df1.dtypes)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469d212b",
   "metadata": {},
   "source": [
    "The model produces high-dimensional vectors, where the size parameter sets the number of dimensions. The optimal number of dimensions depends on the size of the dataset. In our case, 100 dimensions seem to be working very well. min_count parameter controls the minimum frequency of words.\n",
    "\n",
    "\n",
    "\n",
    "https://dylancastillo.co/nlp-snippets-cluster-documents-using-word2vec/\n",
    "\n",
    "\n",
    "## Apply function to remove duplicates\n",
    "\n",
    "\n",
    "Duplicate words can be considered as additional context for the model and can potentially improve the quality of the word embeddings. However, if you have a very large number of duplicates, it may slow down the training process and potentially lead to overfitting. In such cases, it might be beneficial to remove duplicates to speed up training and improve generalization.\n",
    "\n",
    "Overall, whether or not to remove duplicates when using Word2vec will depend on the specific data and the goals of your word embedding task. It is a good idea to try both approaches and see which one gives better results on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e663aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated after preprocessing\n",
    "#_, idx = np.unique(df1[\"tokens\"], return_index=True)\n",
    "#df1 = df1.iloc[idx, :]\n",
    "\n",
    "#print(df1.shape)\n",
    "#df1.head()\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69befde0-d58e-4fbb-ab8f-be187396bc2f",
   "metadata": {},
   "source": [
    "### Check for common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "425fb8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('werden', 246),\n",
       " ('ab', 208),\n",
       " ('Wien', 200),\n",
       " ('Maskenpflicht', 199),\n",
       " ('gelten', 199),\n",
       " ('geben', 192),\n",
       " ('sein', 188),\n",
       " ('mehr', 178),\n",
       " ('Österreich', 165),\n",
       " ('Person', 164)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = df1[\"merged_text\"].tolist()\n",
    "tokenized_docs = df1[\"merged_text\"].map(lambda x: x.split())\n",
    "tokenized_docs = [[token for token in tokens if token != '--']for tokens in tokenized_docs]\n",
    "#tokenized_docs = df1[\"tokens\"].tolist()\n",
    "ids = df1[\"ID_GodotObject\"].tolist()\n",
    "vocab = Counter()\n",
    "for token in tokenized_docs:\n",
    "    vocab.update(token)\n",
    "    \n",
    "vocab.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fc8227-dc0e-4489-a2a4-9e24bb24bda5",
   "metadata": {},
   "source": [
    "### Generate Vectors from document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "511bc2ea-1438-4340-8ad4-5cbc1b97a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(list_of_docs, model):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Embedding\n",
    "\n",
    "    Args:\n",
    "        list_of_docs: List of documents\n",
    "        model: Gensim's Word Embedding\n",
    "\n",
    "    Returns:\n",
    "        List of document vectors\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5859993d-6022-43ee-bd5f-5e0e517dbd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Word2Vec(sentences=tokenized_docs, vector_size=100, workers=1, seed=42)\n",
    "#model.wv.most_similar(\"Ende\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66832f13-ec2d-4880-ba00-e8772c662b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorized_docs = vectorize(tokenized_docs, model=model)\n",
    "#len(vectorized_docs), len(vectorized_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8655f70-1dff-43cd-a462-a407edc09be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mund-nasen-schutz', 0.9989187717437744), ('Supermärkt', 0.9984762072563171), ('weiterhin', 0.9983648657798767), ('Klasse', 0.9983058571815491), ('Ffp2-mask', 0.998256266117096), ('einhalten', 0.9982370138168335), ('dafür', 0.9982022643089294), ('verpflichtend', 0.9981527328491211), ('außerdem', 0.9981510043144226), ('müssen', 0.9981332421302795)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(103, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(sentences=tokenized_docs, vector_size=100, workers=1, seed=42)\n",
    "model.train(tokenized_docs, total_examples=len(tokenized_docs), epochs=10)\n",
    "print(model.wv.most_similar(\"Maske\")) \n",
    "\n",
    "vectorized_docs = vectorize(tokenized_docs, model=model)\n",
    "len(vectorized_docs), len(vectorized_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7182d711-0012-4b55-82bd-0ac38fc53fbe",
   "metadata": {},
   "source": [
    "NOW TRY TO FIT KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9523d002-5c1b-4f64-87ad-479fd96566a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_GodotObject</th>\n",
       "      <th>cluster_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000115059032</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000116305030</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000116325081</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000116346340</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000116371728</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_GodotObject  cluster_value\n",
       "0   2000115059032              5\n",
       "1   2000116305030              3\n",
       "2   2000116325081              6\n",
       "3   2000116346340              4\n",
       "4   2000116371728              2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 7\n",
    "\n",
    "kmeans = KMeans(n_clusters = n)\n",
    "kmeans.fit(vectorized_docs)\n",
    "y_kmeans = kmeans.predict(vectorized_docs)\n",
    "df2 = df1[['ID_GodotObject']].copy()\n",
    "df2['cluster_value'] = y_kmeans\n",
    "df2.to_csv('../../data/feature/knn_clustering.csv', encoding='utf-8', index=False)\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ab7fc-fb54-482d-ba62-57d7e93e659b",
   "metadata": {},
   "source": [
    "now I decided to print the text for each article in each cluster and write the result to a csv file, for later extracting impiortand opinions per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18b62ec8-6a11-4639-bd5e-188886b9152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n):\n",
    "    rslt_df = df2[df2['cluster_value'] == i].copy()\n",
    "    txt_df = df1[['ID_GodotObject','merged_text']].copy()\n",
    "    rslt_df = pd.merge(rslt_df, txt_df, on=\"ID_GodotObject\")\n",
    "    rslt_df.to_csv('../../data/feature/clusters/' + str(i) + '_cluster.csv', encoding='utf-8', index=False)\n",
    "    #print(rslt_df.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d136a3d-0306-494d-a142-8056653a0484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
